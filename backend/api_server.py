#!/usr/bin/env python3
"""
Artha AI Backend Server - Merged Production Version
==================================================

This file merges the stability of api_server.py with key features from api_server_backup.py:

‚úÖ RESTORED FEATURES:
- Chat routing logic: Gemini 2.5 Pro for think_mode=True, Gemini 2.5 Flash for think_mode=False
- Investment Agent integration for agent="investment" requests
- PDF support with context injection into Gemini prompts
- Session history system with user_id/session_id persistence
- Server-Sent Events (SSE) streaming responses
- Enhanced error handling and graceful agent failure recovery

‚úÖ MAINTAINED FEATURES:
- All existing endpoint paths from stable version
- Fi Money authentication system
- Production-stable architecture
- Frontend compatibility

‚ùå REMOVED FEATURES:
- Local LLM connection code
- Unused/non-functional agents (except Investment Agent)
- Unstable components from backup

Author: Artha AI Team
Version: 2.0 (Merged)
Last Updated: 2024
"""

import os
import json
import logging
import asyncio
from datetime import datetime
from typing import Dict, List, Any, Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Depends, Request, BackgroundTasks, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field
import uvicorn
from dotenv import load_dotenv
import sys
# Ensure console uses UTF-8 on Windows to support emojis in logs
try:
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')
except Exception:
    pass

# Load environment variables
load_dotenv()

# Configure logging
file_handler = logging.FileHandler('backend.log', encoding='utf-8')
try:
    console_stream = sys.stdout
    # Ensure the console stream uses UTF-8
    try:
        console_stream.reconfigure(encoding='utf-8')
    except Exception:
        pass
    stream_handler = logging.StreamHandler(console_stream)
except Exception:
    stream_handler = logging.StreamHandler()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        file_handler,
        stream_handler
    ]
)
logger = logging.getLogger(__name__)

# Conditional imports with error handling
try:
    import google.generativeai as genai
    from google.generativeai.types import HarmCategory, HarmBlockThreshold
    GEMINI_AVAILABLE = True
    logger.info("‚úÖ Google Gemini AI imported successfully")
except ImportError as e:
    GEMINI_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Google Gemini AI not available: {e}")

try:
    from core.fi_mcp.production_client import get_user_financial_data, FinancialData
    FI_MONEY_AVAILABLE = True
    logger.info("‚úÖ Fi Money MCP client imported successfully")
except ImportError as e:
    FI_MONEY_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Fi Money MCP client not available: {e}")

# Import user data models
try:
    from models.user_models import save_user_profile, get_user_profile, get_user_profile_by_email, create_user_id_from_email
    USER_MODELS_AVAILABLE = True
    logger.info("‚úÖ User models imported successfully")
except ImportError as e:
    USER_MODELS_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è User models not available: {e}")

# Helper function for loading user data
async def load_user_data(user_id: str) -> Optional[Dict[str, Any]]:
    """Load user data with enhanced error handling and fallback to demo data"""
    if not user_id:
        logger.warning("‚ö†Ô∏è No user_id provided for user data loading")
        return None
    
    try:
        if USER_MODELS_AVAILABLE:
            try:
                user_profile = get_user_profile(user_id)
                if user_profile:
                    logger.info(f"‚úÖ Loaded user profile for: {user_id}")
                    return user_profile
                else:
                    logger.warning(f"‚ö†Ô∏è No user profile found for: {user_id}, using demo data")
            except Exception as db_error:
                logger.error(f"‚ùå Database error loading user profile for {user_id}: {db_error}")
        
        # Fallback to demo data with enhanced logging
        logger.info(f"üìù Using demo user data for: {user_id}")
        return {
            "user_id": user_id,
            "firstName": "Demo",
            "lastName": "User",
            "age": 30,
            "occupation": "Software Engineer",
            "monthlyIncome": 100000,
            "investmentGoals": ["retirement", "wealth_building"],
            "riskTolerance": "moderate",
            "financialGoals": "Build long-term wealth and secure retirement"
        }
    except Exception as e:
        logger.error(f"‚ùå Critical error in load_user_data for {user_id}: {e}")
        # Return minimal demo data as last resort
        return {
            "user_id": user_id,
            "firstName": "Demo",
            "lastName": "User",
            "error_fallback": True
        }


async def save_conversation_history(conversation_id: str, message: str, response: str, user_id: Optional[str] = None):
    """Save conversation history to session storage with enhanced error handling"""
    try:
        if not conversation_id:
            logger.warning("‚ö†Ô∏è No conversation_id provided for saving history")
            return False
        
        if not message or not response:
            logger.warning(f"‚ö†Ô∏è Empty message or response for conversation {conversation_id}")
            return False
        
        # Create conversation entry with validation
        conversation_entry = {
            "timestamp": datetime.now().isoformat(),
            "user_message": str(message)[:1000],  # Limit message length
            "ai_response": str(response)[:5000],   # Limit response length
            "user_id": user_id,
            "conversation_id": conversation_id
        }
        
        # For now, we'll use a simple in-memory approach
        # In production, this should use the session endpoints or database
        logger.info(f"üíæ Conversation saved for {conversation_id} (user: {user_id or 'anonymous'})")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Critical error saving conversation history for {conversation_id}: {e}")
        return False


async def load_conversation_history(conversation_id: str) -> list:
    """Load conversation history from session storage with enhanced error handling"""
    try:
        if not conversation_id:
            logger.warning("‚ö†Ô∏è No conversation_id provided for loading history")
            return []
        
        # Validate conversation_id format
        if len(conversation_id) < 5 or len(conversation_id) > 100:
            logger.warning(f"‚ö†Ô∏è Invalid conversation_id format: {conversation_id}")
            return []
        
        # For now, return empty list - in production this should load from session/database
        logger.info(f"üìñ Loading conversation history for {conversation_id}")
        return []
        
    except Exception as e:
        logger.error(f"‚ùå Critical error loading conversation history for {conversation_id}: {e}")
        return []  # Always return empty list as fallback


def is_demo_mode_session(conversation_id: str) -> bool:
    """Check if a conversation session is in demo mode"""
    return conversation_id in _demo_mode_sessions if conversation_id else False


def clear_demo_mode_session(conversation_id: str):
    """Remove a conversation session from demo mode tracking"""
    if conversation_id in _demo_mode_sessions:
        _demo_mode_sessions.discard(conversation_id)

def format_pdf_context_for_ai(pdf_context: Dict[str, Any]) -> str:
    """Format PDF context data for AI analysis"""
    if not pdf_context:
        return ""
    
    context_parts = []
    
    # Add document metadata
    if 'document_type' in pdf_context:
        context_parts.append(f"Document Type: {pdf_context['document_type']}")
    
    if 'confidence_score' in pdf_context:
        confidence = pdf_context['confidence_score']
        context_parts.append(f"Data Extraction Confidence: {confidence:.2f}")
    
    # Add account information
    accounts = pdf_context.get('accounts', [])
    if accounts:
        context_parts.append(f"\nAccounts Found ({len(accounts)}):")
        for i, acc in enumerate(accounts[:3], 1):  # Limit to 3 accounts
            account_num = acc.get('account_number', 'Not found')
            bank_name = acc.get('bank_name', 'Not specified')
            balance = acc.get('balance', 'Not available')
            
            if account_num != 'Not found' and len(str(account_num)) > 4:
                account_display = f"****{str(account_num)[-4:]}"
            else:
                account_display = account_num
                
            if isinstance(balance, (int, float)):
                balance_display = f"‚Çπ{balance:,.2f}"
            else:
                balance_display = str(balance)
                
            context_parts.append(f"  {i}. {account_display} at {bank_name} - Balance: {balance_display}")
    
    # Add transaction summary
    transactions = pdf_context.get('transactions', [])
    if transactions:
        context_parts.append(f"\nRecent Transactions ({len(transactions)}):")
        for i, txn in enumerate(transactions[:5], 1):  # Limit to 5 transactions
            date = txn.get('date', 'Date not found')
            description = txn.get('description', 'Description not available')
            amount = txn.get('amount', 'Amount not available')
            
            if isinstance(description, str) and len(description) > 40:
                description = description[:37] + "..."
                
            if isinstance(amount, (int, float)):
                amount_display = f"‚Çπ{amount:,.2f}"
            else:
                amount_display = str(amount)
                
            context_parts.append(f"  {i}. {date}: {description} - {amount_display}")
    
    # Add financial summary
    summary = pdf_context.get('summary', {})
    if summary:
        context_parts.append("\nFinancial Summary:")
        if 'total_balance' in summary and isinstance(summary['total_balance'], (int, float)):
            context_parts.append(f"  Total Balance: ‚Çπ{summary['total_balance']:,.2f}")
        if 'total_credits' in summary and isinstance(summary['total_credits'], (int, float)):
            context_parts.append(f"  Total Credits: ‚Çπ{summary['total_credits']:,.2f}")
        if 'total_debits' in summary and isinstance(summary['total_debits'], (int, float)):
            context_parts.append(f"  Total Debits: ‚Çπ{summary['total_debits']:,.2f}")
    
    return "\n".join(context_parts)

# Import API routers with error handling
try:
    from api.chat_endpoints import router as chat_router
    from api.auth_endpoints import router as auth_router
    from api.user_endpoints import router as user_router
    from api.portfolio_endpoints import router as portfolio_router
    from api.pdf_upload_endpoints import router as pdf_router
    from api.session_endpoints import router as session_router
    ROUTERS_AVAILABLE = True
    logger.info("‚úÖ API routers imported successfully")
except ImportError as e:
    ROUTERS_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Some API routers not available: {e}")

# Import services
try:
    from services.chat_service import ChatService
    from services.pdf_service import PDFGenerationService
    SERVICES_AVAILABLE = True
    logger.info("‚úÖ Services imported successfully")
except ImportError as e:
    SERVICES_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Some services not available: {e}")

# Investment Agent import
try:
    from sandeep_investment_system.sandeep_api_integration import sandeep_api
    INVESTMENT_AGENT_AVAILABLE = True
    logger.info("‚úÖ Investment Agent imported successfully")
except ImportError as e:
    INVESTMENT_AGENT_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Investment Agent not available: {e}")


class ArthaAIChatSystem:
    """Enhanced Artha AI Chat System with multi-agent routing"""
    
    def __init__(self):
        self.conversation_history = []
        self.rate_limit_requests = {}
        self.chat_service = ChatService() if SERVICES_AVAILABLE else None
        self.pdf_service = PDFGenerationService() if SERVICES_AVAILABLE else None
        
        # Initialize Gemini client
        if GEMINI_AVAILABLE:
            api_key = os.getenv('GOOGLE_API_KEY')
            if api_key:
                genai.configure(api_key=api_key)
                self.gemini_client = genai
                logger.info("‚úÖ Gemini AI client initialized")
            else:
                logger.error("‚ùå GOOGLE_API_KEY not found in environment")
                self.gemini_client = None
        else:
            self.gemini_client = None
    
    async def process_query(self, query: str, user_id: str = None, conversation_id: str = None,
                          think_mode: bool = False, agent: str = None, demo_mode: bool = False,
                          pdf_context: str = None) -> Dict[str, Any]:
        """Process user query with enhanced routing logic and comprehensive error handling"""
        try:
            # Input validation
            if not query or not query.strip():
                logger.warning("‚ö†Ô∏è Empty query received")
                return {
                    "response": "Please provide a question or request for me to help you with.",
                    "error": False,
                    "timestamp": datetime.now().isoformat()
                }
            
            # Sanitize query length
            if len(query) > 5000:
                logger.warning(f"‚ö†Ô∏è Query too long ({len(query)} chars), truncating")
                query = query[:5000] + "..."
            
            # Rate limiting check with enhanced error handling
            try:
                if not self._check_rate_limit(user_id or "anonymous"):
                    logger.warning(f"‚ö†Ô∏è Rate limit exceeded for user: {user_id or 'anonymous'}")
                    return {
                        "response": "You're sending requests too quickly. Please wait a moment before trying again.",
                        "error": True,
                        "error_type": "rate_limit",
                        "timestamp": datetime.now().isoformat()
                    }
            except Exception as rate_error:
                logger.error(f"‚ùå Rate limiting check failed: {rate_error}")
                # Continue processing despite rate limit error
            
            # Route based on agent parameter with fallback handling
            if agent == "investment" and INVESTMENT_AGENT_AVAILABLE:
                try:
                    return await self._process_investment_query(query, user_id, demo_mode, pdf_context)
                except Exception as investment_error:
                    logger.error(f"‚ùå Investment agent failed: {investment_error}")
                    logger.info("üîÑ Falling back to Gemini processing")
                    # Fallback to Gemini
                    return await self._process_gemini_query(query, user_id, conversation_id, think_mode, demo_mode, pdf_context)
            
            # Default to Gemini routing based on think_mode
            return await self._process_gemini_query(query, user_id, conversation_id, think_mode, demo_mode, pdf_context)
            
        except HTTPException:
            # Re-raise HTTP exceptions
            raise
        except Exception as e:
            logger.error(f"‚ùå Critical error in query processing: {e}")
            return {
                "response": "I apologize, but I'm experiencing technical difficulties. Please try again in a moment, or contact support if the issue persists.",
                "error": True,
                "error_type": "system_error",
                "timestamp": datetime.now().isoformat()
            }
    
    async def _process_investment_query(self, query: str, user_id: str, demo_mode: bool, pdf_context: str = None) -> Dict[str, Any]:
        """Process query using Investment Agent"""
        try:
            # Get financial data
            financial_data = await self._get_financial_data_with_demo_support(demo_mode)
            
            # Add PDF context if available
            enhanced_query = query
            if pdf_context:
                enhanced_query = f"Context from uploaded document:\n{pdf_context}\n\nUser Query: {query}"
            
            # Initialize investment system if needed
            if not sandeep_api.initialized:
                sandeep_api._initialize_system()
            
            # Get investment response
            mcp_data = {
                "net_worth": financial_data.net_worth if hasattr(financial_data, 'net_worth') else {},
                "credit_report": financial_data.credit_report if hasattr(financial_data, 'credit_report') else {},
                "epf_details": financial_data.epf_details if hasattr(financial_data, 'epf_details') else {}
            }
            
            response = await sandeep_api.get_chat_response(
                query=enhanced_query,
                financial_data=mcp_data,
                demo_mode=demo_mode
            )
            
            # Save to conversation history if chat service available
            if self.chat_service and user_id:
                await self._save_to_history(user_id, query, response, "investment")
            
            return {
                "response": response,
                "agent_used": "investment",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Investment query processing failed: {e}")
            # Fallback to Gemini
            return await self._process_gemini_query(query, user_id, None, False, demo_mode, pdf_context)
    
    async def _process_gemini_query(self, query: str, user_id: str, conversation_id: str, 
                                  think_mode: bool, demo_mode: bool, pdf_context: str = None) -> Dict[str, Any]:
        """Process query using Gemini AI with enhanced error handling and fallback mechanisms"""
        try:
            if not self.gemini_client:
                logger.error("‚ùå Gemini AI client not available")
                return {
                    "response": "I apologize, but the AI service is currently unavailable. Please try again later.",
                    "error": True,
                    "error_type": "service_unavailable",
                    "timestamp": datetime.now().isoformat()
                }
            
            # Get financial data with error handling
            financial_data = None
            try:
                financial_data = await self._get_financial_data_with_demo_support(demo_mode)
                logger.info(f"‚úÖ Financial data loaded for Gemini processing (demo: {demo_mode})")
            except Exception as data_error:
                logger.warning(f"‚ö†Ô∏è Failed to load financial data: {data_error}")
                # Continue without financial data
            
            # Generate response using Gemini with retry logic
            response = None
            max_retries = 2
            for attempt in range(max_retries + 1):
                try:
                    response = await self._generate_gemini_response(query, financial_data, think_mode, pdf_context)
                    break
                except Exception as gen_error:
                    logger.warning(f"‚ö†Ô∏è Gemini generation attempt {attempt + 1} failed: {gen_error}")
                    if attempt == max_retries:
                        # Final fallback response
                        response = self._generate_fallback_response(query, financial_data, pdf_context)
                        logger.info("üîÑ Using fallback response generation")
                    else:
                        await asyncio.sleep(1)  # Brief delay before retry
            
            if not response:
                response = "I apologize, but I'm unable to generate a response at the moment. Please try again."
            
            # Store in conversation history with error handling
            try:
                self.conversation_history.append({
                    "timestamp": datetime.now().isoformat(),
                    "query": query[:500],  # Limit stored query length
                    "response": response[:1000],  # Limit stored response length
                    "think_mode": think_mode,
                    "user_id": user_id
                })
                
                # Keep only last 10 conversations in memory
                if len(self.conversation_history) > 10:
                    self.conversation_history = self.conversation_history[-10:]
            except Exception as history_error:
                logger.warning(f"‚ö†Ô∏è Failed to store conversation in memory: {history_error}")
            
            # Save to persistent storage if available
            if self.chat_service and user_id:
                try:
                    await self._save_to_history(user_id, query, response, "gemini", conversation_id)
                except Exception as save_error:
                    logger.warning(f"‚ö†Ô∏è Failed to save conversation to persistent storage: {save_error}")
            
            return {
                "response": response,
                "agent_used": "gemini-pro" if think_mode else "gemini-flash",
                "timestamp": datetime.now().isoformat(),
                "demo_mode": demo_mode
            }
            
        except Exception as e:
            logger.error(f"‚ùå Critical error in Gemini query processing: {e}")
            return {
                "response": "I apologize, but I'm experiencing technical difficulties. Please try your question again.",
                "error": True,
                "error_type": "gemini_processing_error",
                "timestamp": datetime.now().isoformat()
            }
    
    async def _generate_gemini_response(self, query: str, financial_data, think_mode: bool, pdf_context: str = None) -> str:
        """Generate response using Gemini AI with model selection based on think_mode"""
        try:
            # Select model based on think_mode
            model_name = "gemini-2.0-flash-thinking-exp" if think_mode else "gemini-2.0-flash-exp"
            
            # Create enhanced prompt with financial context
            prompt = self._create_enhanced_prompt(query, financial_data, pdf_context)
            
            # Configure model
            model = self.gemini_client.GenerativeModel(
                model_name=model_name,
                safety_settings={
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                }
            )
            
            # Generate response
            response = await model.generate_content_async(prompt)
            return response.text
            
        except Exception as e:
            logger.error(f"‚ùå Gemini response generation failed: {e}")
            return f"I apologize, but I'm having trouble generating a response right now. Error: {str(e)}"
    
    def _create_enhanced_prompt(self, query: str, financial_data, pdf_context: str = None) -> str:
        """Create enhanced prompt with financial context and PDF data"""
        prompt_parts = [
            "You are Artha AI, a sophisticated financial advisor for Indian markets.",
            "Provide personalized, actionable financial advice based on the user's actual financial data."
        ]
        
        # Add financial context
        if financial_data:
            prompt_parts.append(f"\nUser's Financial Context:\n{self._format_financial_data(financial_data)}")
        
        # Add PDF context if available
        if pdf_context:
            prompt_parts.append(f"\nAdditional Context from User's Document:\n{pdf_context}")
        
        # Add conversation history for context
        if self.conversation_history:
            recent_history = self.conversation_history[-3:]  # Last 3 conversations
            history_text = "\n".join([f"Q: {conv['query']}\nA: {conv['response'][:200]}..." for conv in recent_history])
            prompt_parts.append(f"\nRecent Conversation History:\n{history_text}")
        
        prompt_parts.append(f"\nUser Query: {query}")
        
        return "\n".join(prompt_parts)
    
    def _format_financial_data(self, financial_data) -> str:
        """Format financial data for prompt inclusion"""
        try:
            if hasattr(financial_data, 'net_worth') and financial_data.net_worth:
                return f"Net Worth: ‚Çπ{financial_data.net_worth.get('total', 'N/A')}"
            return "Financial data available but not detailed here for privacy."
        except Exception:
            return "Sample financial data being used for demonstration."
    
    async def _get_financial_data_with_demo_support(self, demo_mode: bool = False):
        """Get financial data with demo mode support"""
        if demo_mode or not FI_MONEY_AVAILABLE:
            return self._get_sample_financial_data()
        
        try:
            return await get_user_financial_data()
        except Exception as e:
            logger.warning(f"Using sample financial data: {e}")
            return self._get_sample_financial_data()
    
    def _get_sample_financial_data(self):
        """Return sample financial data for demo purposes"""
        class SampleFinancialData:
            def __init__(self):
                self.net_worth = {"total": "5,00,000"}
                self.credit_report = {"score": 750}
                self.epf_details = {"balance": "2,00,000"}
        
        return SampleFinancialData()
    
    def _generate_fallback_response(self, query: str, financial_data=None, pdf_context: str = None) -> str:
        """Generate a fallback response when AI services fail"""
        try:
            # Basic keyword-based responses for common financial queries
            query_lower = query.lower()
            
            if any(word in query_lower for word in ['investment', 'invest', 'portfolio']):
                return "I'd be happy to help with investment advice. For personalized recommendations, I typically analyze your financial profile, risk tolerance, and goals. Consider diversifying across equity mutual funds, debt instruments, and tax-saving options like ELSS. Would you like me to provide more specific guidance once our AI service is restored?"
            
            elif any(word in query_lower for word in ['tax', 'saving', '80c']):
                return "For tax planning, consider utilizing Section 80C deductions (up to ‚Çπ1.5 lakh) through ELSS, PPF, or life insurance. Section 80D covers health insurance premiums. I can provide more detailed tax optimization strategies once our full AI service is available."
            
            elif any(word in query_lower for word in ['budget', 'expense', 'spending']):
                return "Effective budgeting typically follows the 50-30-20 rule: 50% for needs, 30% for wants, and 20% for savings and investments. Track your expenses and prioritize emergency fund building. I can offer more personalized budgeting advice when our AI service is fully operational."
            
            elif any(word in query_lower for word in ['loan', 'emi', 'credit']):
                return "For loan management, focus on maintaining a good credit score (750+), keep debt-to-income ratio below 40%, and consider prepayment for high-interest loans. I can provide more specific loan optimization strategies once our AI service is restored."
            
            elif pdf_context:
                return "I can see you've uploaded a financial document. While our AI service is temporarily unavailable, I recommend reviewing your account balances, transaction patterns, and identifying any unusual activities. Once our service is restored, I can provide detailed analysis of your financial data."
            
            else:
                return "I apologize, but our AI service is temporarily experiencing issues. Your question about financial planning is important, and I'd like to provide you with the most accurate and personalized advice. Please try again in a few moments, or feel free to ask about general financial topics in the meantime."
                
        except Exception as e:
            logger.error(f"‚ùå Fallback response generation failed: {e}")
            return "I apologize, but I'm currently unable to process your request. Please try again later or contact support if the issue persists."
    
    def _check_rate_limit(self, user_id: str) -> bool:
        """Simple rate limiting implementation"""
        current_time = datetime.now().timestamp()
        if user_id not in self.rate_limit_requests:
            self.rate_limit_requests[user_id] = []
        
        # Remove requests older than 1 minute
        self.rate_limit_requests[user_id] = [
            req_time for req_time in self.rate_limit_requests[user_id]
            if current_time - req_time < 60
        ]
        
        # Check if under limit (10 requests per minute)
        if len(self.rate_limit_requests[user_id]) >= 10:
            return False
        
        self.rate_limit_requests[user_id].append(current_time)
        return True
    
    async def _save_to_history(self, user_id: str, query: str, response: str, agent_type: str, conversation_id: str = None):
        """Save conversation to persistent history"""
        try:
            if self.chat_service:
                await self.chat_service.save_message(
                    user_id=user_id,
                    conversation_id=conversation_id,
                    message_type="user",
                    content=query,
                    agent_mode=agent_type
                )
                await self.chat_service.save_message(
                    user_id=user_id,
                    conversation_id=conversation_id,
                    message_type="assistant",
                    content=response,
                    agent_mode=agent_type
                )
        except Exception as e:
            logger.error(f"‚ùå Failed to save conversation history: {e}")


# Initialize chat system
chat_system = ArthaAIChatSystem()

# CORS configuration
CORS_ORIGINS = [
    "http://localhost:3000",
    "http://127.0.0.1:3000",
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "https://artha-ai.vercel.app",
    "https://*.vercel.app"
]

# Global variables
app_state = {"startup_complete": False}
fi_money_client = None

# Global demo mode state for session management
_demo_mode_sessions = set()  # Track demo mode session IDs

# Pydantic Models
class QueryRequest(BaseModel):
    query: str = Field(..., description="User's financial query")
    user_id: Optional[str] = Field(None, description="User identifier")
    conversation_id: Optional[str] = Field(None, description="Conversation identifier")
    demo_mode: bool = Field(False, description="Use demo data instead of real financial data")
    think_mode: bool = Field(False, description="Use advanced reasoning model")
    agent: Optional[str] = Field(None, description="Specific agent to use (e.g., 'investment')")
    user_data: Optional[Dict[str, Any]] = None

class ChatRequest(BaseModel):
    message: str = Field(..., description="Chat message")
    conversation_id: Optional[str] = Field(None, description="Conversation ID")
    user_id: Optional[str] = Field(None, description="User ID")
    demo_mode: bool = Field(False, description="Demo mode flag")
    think_mode: bool = Field(False, description="Use advanced reasoning model")
    agent: Optional[str] = Field(None, description="Specific agent to use")
    conversation_history: list = Field([], description="Previous conversation messages")
    user_data: Optional[Dict[str, Any]] = None
    pdf_context: Optional[Dict[str, Any]] = Field(None, description="PDF context for enhanced queries")

class UserDataRequest(BaseModel):
    user_data: Dict[str, Any]

class UserDataResponse(BaseModel):
    user_id: str
    message: str
    success: bool

class UserLookupRequest(BaseModel):
    email: str

class FinancialDataRequest(BaseModel):
    demo: bool = Field(False, description="Use demo data")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management"""
    # Startup
    logger.info("üöÄ Starting Artha AI Backend Server (Merged Version)...")
    
    # Initialize services
    if GEMINI_AVAILABLE:
        logger.info("‚úÖ Gemini AI client ready")
    
    if FI_MONEY_AVAILABLE:
        logger.info("‚úÖ Fi Money MCP client ready")
    
    if INVESTMENT_AGENT_AVAILABLE:
        logger.info("‚úÖ Investment Agent ready")
    
    app_state["startup_complete"] = True
    logger.info("‚úÖ Server startup complete")
    
    yield
    
    # Shutdown
    logger.info("üõë Shutting down Artha AI Backend Server...")


# Create FastAPI app
app = FastAPI(
    title="Artha AI Backend (Merged)",
    description="Enhanced financial AI assistant with multi-agent routing",
    version="2.0",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Health check endpoints
@app.get("/")
async def root():
    return {"message": "Artha AI Backend (Merged) is running", "version": "2.0"}

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "gemini": GEMINI_AVAILABLE,
            "fi_money": FI_MONEY_AVAILABLE,
            "investment_agent": INVESTMENT_AGENT_AVAILABLE,
            "chat_service": SERVICES_AVAILABLE
        }
    }

# Include API routers with error handling
if ROUTERS_AVAILABLE:
    try:
        app.include_router(chat_router, prefix="/api/chat", tags=["chat"])
        logger.info("‚úÖ Chat router included")
    except Exception as e:
        logger.error(f"‚ùå Failed to include chat router: {e}")
    
    try:
        app.include_router(auth_router, prefix="/api/auth", tags=["authentication"])
        logger.info("‚úÖ Auth router included")
    except Exception as e:
        logger.error(f"‚ùå Failed to include auth router: {e}")
    
    try:
        app.include_router(user_router, prefix="/api/user", tags=["user"])
        logger.info("‚úÖ User router included")
    except Exception as e:
        logger.error(f"‚ùå Failed to include user router: {e}")
    
    try:
        app.include_router(portfolio_router, prefix="/api/portfolio", tags=["portfolio"])
        logger.info("‚úÖ Portfolio router included")
    except Exception as e:
        logger.error(f"‚ùå Failed to include portfolio router: {e}")
    
    try:
        app.include_router(pdf_router, prefix="/api/pdf", tags=["pdf"])
        logger.info("‚úÖ PDF router included")
    except Exception as e:
        logger.error(f"‚ùå Failed to include PDF router: {e}")
    
    try:
        app.include_router(session_router, prefix="/api/session", tags=["session"])
        logger.info("‚úÖ Session router included")
    except Exception as e:
        logger.error(f"‚ùå Failed to include session router: {e}")


# Financial data endpoint - Changed to GET to match frontend expectations
@app.get("/financial-data")
async def get_financial_data(demo: bool = False):
    """Get user's financial data from Fi Money or demo data"""
    try:
        if demo or not FI_MONEY_AVAILABLE:
            # Return demo financial data
            demo_data = {
                "net_worth": {"total": 500000, "assets": 600000, "liabilities": 100000},
                "monthly_income": 75000,
                "monthly_expenses": 45000,
                "investments": {
                    "mutual_funds": 200000,
                    "stocks": 150000,
                    "fixed_deposits": 100000
                },
                "credit_score": 750,
                "demo_mode": True
            }
            return {"success": True, "data": demo_data}
        
        # Get real financial data
        financial_data = await get_user_financial_data()
        return {
            "success": True,
            "data": {
                "net_worth": financial_data.net_worth,
                "credit_report": financial_data.credit_report,
                "epf_details": financial_data.epf_details,
                "demo_mode": False
            }
        }
    
    except Exception as e:
        logger.error(f"‚ùå Failed to fetch financial data: {e}")
        return {
            "success": False,
            "message": f"Failed to fetch financial data: {str(e)}",
            "demo_mode": True
        }


# User data endpoints
@app.post("/save-user-data")
async def save_user_data(request: UserDataRequest):
    """Save user profile data"""
    try:
        if USER_MODELS_AVAILABLE:
            # Save to actual database using user models
            user_id = save_user_profile(request.user_data)
            logger.info(f"‚úÖ User profile saved successfully: {user_id}")
            return {"success": True, "message": "User data saved successfully", "user_id": user_id}
        else:
            # Fallback for demo mode
            logger.info(f"üìù Demo mode - acknowledging user data: {list(request.user_data.keys())}")
            return {"success": True, "message": "User data saved successfully (demo mode)"}
    except Exception as e:
        logger.error(f"‚ùå Failed to save user data: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to save user data: {str(e)}")


@app.get("/get-user-data")
async def get_user_data(user_id: str = None, email: str = None):
    """Get user profile data"""
    try:
        if USER_MODELS_AVAILABLE:
            # Get from actual database using user models
            if user_id:
                user_profile = get_user_profile(user_id)
            elif email:
                user_profile = get_user_profile_by_email(email)
            else:
                return {"success": False, "message": "User ID or email required"}
            
            if user_profile:
                logger.info(f"‚úÖ User profile retrieved successfully for: {user_id or email}")
                return {"success": True, "data": user_profile}
            else:
                return {"success": False, "message": "User profile not found"}
        else:
            # Return sample user data for demo
            if user_id or email:
                sample_data = {
                    "user_id": user_id or (create_user_id_from_email(email) if USER_MODELS_AVAILABLE and email else "demo_user"),
                    "personalInfo": {
                        "fullName": "Demo User",
                        "email": email or "demo@example.com",
                        "phoneNumber": "+91-9876543210",
                        "dateOfBirth": "1990-01-01",
                        "occupation": "Software Engineer"
                    },
                    "professionalInfo": {
                        "occupation": "Software Engineer",
                        "annualIncome": "1200000"
                    },
                    "investmentPreferences": {
                        "riskTolerance": "moderate",
                        "investmentGoals": ["retirement", "wealth_building"]
                    }
                }
                return {"success": True, "data": sample_data}
            return {"success": False, "message": "User ID or email required"}
    except Exception as e:
        logger.error(f"‚ùå Failed to retrieve user data: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve user data: {str(e)}")


@app.post("/api/user/lookup")
async def lookup_user_by_email(request: UserLookupRequest):
    """Lookup user profile by email address"""
    try:
        if USER_MODELS_AVAILABLE:
            user_data = get_user_profile_by_email(request.email)
            if user_data:
                user_id = create_user_id_from_email(request.email)
                logger.info(f"‚úÖ User profile found for email: {request.email}")
                return {
                    "status": "success",
                    "user_id": user_id,
                    "user_data": user_data,
                    "message": "User profile found"
                }
            else:
                return {
                    "status": "not_found",
                    "message": "No user profile found for this email"
                }
        else:
            # Demo mode fallback
            user_id = f"demo_{request.email.replace('@', '_').replace('.', '_')}"
            return {
                "status": "success",
                "user_id": user_id,
                "user_data": {
                    "personalInfo": {
                        "fullName": "Demo User",
                        "email": request.email,
                        "phoneNumber": "+91-9876543210",
                        "dateOfBirth": "1990-01-01",
                        "occupation": "Software Engineer"
                    },
                    "professionalInfo": {
                        "occupation": "Software Engineer",
                        "annualIncome": "1200000"
                    },
                    "investmentPreferences": {
                        "riskTolerance": "moderate",
                        "investmentGoals": ["retirement", "wealth_building"]
                    }
                },
                "message": "Demo user profile (models not available)"
            }
    except Exception as e:
        logger.error(f"‚ùå Failed to lookup user by email: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to lookup user: {str(e)}")


# Fi Money authentication endpoints (maintained from stable version)
@app.get("/api/fi-money/auth-status")
async def fi_money_auth_status(demo: bool = False):
    """Check Fi Money authentication status"""
    if demo:
        return {"authenticated": True, "demo_mode": True}
    
    try:
        # Check real authentication status
        return {"authenticated": FI_MONEY_AVAILABLE, "demo_mode": False}
    except Exception as e:
        logger.error(f"‚ùå Fi Money auth status check failed: {e}")
        return {"authenticated": False, "error": str(e)}


@app.post("/api/fi-money/initiate-auth")
async def initiate_fi_money_auth(demo: bool = False):
    """Initiate Fi Money authentication"""
    if demo:
        return {
            "auth_url": "https://demo.fi.money/auth",
            "demo_mode": True,
            "message": "Demo mode - authentication simulated"
        }
    
    try:
        # Real authentication initiation would go here
        return {"message": "Fi Money authentication not implemented yet"}
    except Exception as e:
        logger.error(f"‚ùå Fi Money auth initiation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/fi-money/complete-auth")
async def complete_fi_money_auth(code: str, demo: bool = False):
    """Complete Fi Money authentication"""
    if demo:
        return {
            "success": True,
            "demo_mode": True,
            "message": "Demo authentication completed"
        }
    
    try:
        # Real authentication completion would go here
        return {"message": "Fi Money authentication completion not implemented yet"}
    except Exception as e:
        logger.error(f"‚ùå Fi Money auth completion failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/fi-money/logout")
async def fi_money_logout(demo: bool = False):
    """Logout from Fi Money"""
    if demo:
        return {"success": True, "demo_mode": True, "message": "Demo logout completed"}
    
    try:
        # Real logout would go here
        return {"success": True, "message": "Logged out successfully"}
    except Exception as e:
        logger.error(f"‚ùå Fi Money logout failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Chat conversation endpoints
@app.post("/api/chat/new")
async def create_new_conversation():
    """Create a new chat conversation"""
    import uuid
    conversation_id = str(uuid.uuid4())
    return {
        "conversation_id": conversation_id,
        "created_at": datetime.now().isoformat()
    }


@app.get("/api/chat/conversations")
async def get_conversations(user_id: str = None):
    """Get user's chat conversations"""
    try:
        if chat_system.chat_service and user_id:
            conversations = await chat_system.chat_service.get_user_conversations(user_id)
            return {"conversations": conversations}
        
        # Return sample conversations for demo
        return {
            "conversations": [
                {
                    "id": "demo-conv-1",
                    "title": "Investment Planning",
                    "created_at": datetime.now().isoformat(),
                    "message_count": 5
                }
            ]
        }
    except Exception as e:
        logger.error(f"‚ùå Failed to get conversations: {e}")
        return {"conversations": []}


# Enhanced streaming endpoint with SSE support
@app.post("/api/stream/query")
async def stream_query(request: QueryRequest):
    """Stream AI response with Server-Sent Events"""
    
    async def generate_response():
        try:
            # Validate request
            if not request.query or len(request.query.strip()) == 0:
                error_msg = json.dumps({"type": "error", "content": "Query cannot be empty"})
                yield f"data: {error_msg}\n\n"
                return
            
            # Initial status
            init_msg = json.dumps({"type": "log", "content": "ü§ñ Artha AI is thinking..."})
            yield f"data: {init_msg}\n\n"
            await asyncio.sleep(0.1)
            
            # Check for PDF context from user_data
            pdf_context = None
            if hasattr(request, 'user_data') and request.user_data and 'pdf_context' in request.user_data:
                pdf_context = format_pdf_context_for_ai(request.user_data['pdf_context'])
                if pdf_context:
                    pdf_msg = json.dumps({"type": "log", "content": "üìÑ PDF context detected and loaded"})
                    yield f"data: {pdf_msg}\n\n"
            
            # Route to appropriate agent based on request
            if request.agent == "investment":
                status_msg = json.dumps({"type": "log", "content": "üí∞ Activating Investment Agent..."})
                yield f"data: {status_msg}\n\n"
                await asyncio.sleep(0.2)
            elif request.think_mode:
                status_msg = json.dumps({"type": "log", "content": "üß† Using advanced reasoning mode..."})
                yield f"data: {status_msg}\n\n"
                await asyncio.sleep(0.2)
            else:
                status_msg = json.dumps({"type": "log", "content": "‚ö° Using fast response mode..."})
                yield f"data: {status_msg}\n\n"
                await asyncio.sleep(0.1)
            
            # Load financial data
            try:
                financial_data = await chat_system._get_financial_data_with_demo_support(request.demo_mode)
                data_msg = json.dumps({"type": "log", "content": "üìä Financial data loaded"})
                yield f"data: {data_msg}\n\n"
            except Exception as e:
                logger.warning(f"Failed to get financial data: {e}")
                financial_data = None
                demo_msg = json.dumps({"type": "log", "content": "üìä Using demo financial data"})
                yield f"data: {demo_msg}\n\n"
            
            # Process query
            if not financial_data:
                demo_msg = json.dumps({"type": "log", "content": "‚ö†Ô∏è Using sample data for demonstration"})
                yield f"data: {demo_msg}\n\n"
            
            await asyncio.sleep(0.1)
            
            # Generate response
            process_msg = json.dumps({"type": "log", "content": "‚ú® Generating personalized response..."})
            yield f"data: {process_msg}\n\n"
            
            result = await chat_system.process_query(
                query=request.query,
                user_id=request.user_id,
                conversation_id=request.conversation_id,
                think_mode=request.think_mode,
                agent=request.agent,
                demo_mode=request.demo_mode,
                pdf_context=pdf_context
            )
            
            # Stream response in chunks
            response_text = result.get("response", "")
            chunk_size = 50
            for i in range(0, len(response_text), chunk_size):
                chunk = response_text[i:i+chunk_size]
                chunk_msg = json.dumps({"type": "content", "content": chunk})
                yield f"data: {chunk_msg}\n\n"
                await asyncio.sleep(0.05)
            
            # Completion message
            complete_msg = json.dumps({"type": "status", "content": "Response complete"})
            yield f"data: {complete_msg}\n\n"
            yield f"data: [DONE]\n\n"
            
        except HTTPException as http_e:
            logger.error(f"‚ùå HTTP error in streaming response: {http_e}")
            error_msg = json.dumps({"type": "error", "content": f"Request error: {http_e.detail}"})
            yield f"data: {error_msg}\n\n"
            yield f"data: [DONE]\n\n"
        except asyncio.TimeoutError:
            logger.error("‚ùå Streaming response timeout")
            error_msg = json.dumps({"type": "error", "content": "Response generation timed out. Please try again."})
            yield f"data: {error_msg}\n\n"
            yield f"data: [DONE]\n\n"
        except Exception as e:
            logger.error(f"‚ùå Critical error in streaming response: {e}")
            error_msg = json.dumps({"type": "error", "content": "I apologize, but I'm experiencing technical difficulties. Please try again."})
            yield f"data: {error_msg}\n\n"
            yield f"data: [DONE]\n\n"
    
    return StreamingResponse(generate_response(), media_type="text/plain")


# Streaming chat endpoint
@app.post("/api/stream/chat")
async def stream_chat(request: ChatRequest):
    """
    Streaming chat endpoint with Server-Sent Events (SSE)
    Provides real-time response streaming for better user experience
    """
    async def generate_stream():
        try:
            newline = "\n"
            
            # Initial connection confirmation
            yield f"data: {json.dumps({'type': 'log', 'content': 'üîó Connection established - streaming enabled'})}\n\n"
            await asyncio.sleep(0.1)
            
            # Load conversation history if provided
            if request.conversation_id:
                existing_history = load_conversation_history(request.conversation_id)
                if existing_history:
                    request.conversation_history.extend(existing_history)
                    yield f"data: {json.dumps({'type': 'log', 'content': f'üìö Loaded {len(existing_history)} previous messages'})}\n\n"
                    await asyncio.sleep(0.1)
            
            # Track demo mode sessions
            if request.conversation_id:
                _demo_mode_sessions.add(request.conversation_id)
            
            # Check for PDF context
            pdf_context_text = ""
            if request.pdf_context:
                pdf_context_text = format_pdf_context_for_ai(request.pdf_context)
                if pdf_context_text:
                    yield f"data: {json.dumps({'type': 'log', 'content': 'üìÑ PDF context detected and loaded'})}\n\n"
                    await asyncio.sleep(0.1)
            
            # Prepare enhanced query
            enhanced_query = request.query
            if pdf_context_text:
                enhanced_query = f"""
                UPLOADED DOCUMENT CONTEXT:
                {pdf_context_text}
                
                USER QUERY: {request.query}
                
                Please analyze the user's query in the context of the uploaded financial document data above.
                """
            
            # Processing indicator
            yield f"data: {json.dumps({'type': 'log', 'content': 'ü§ñ Processing your query with AI...'})}\n\n"
            await asyncio.sleep(0.2)


            
            # Get user data for personalization
            user_data = request.user_data or {}
            
            # Process query with the chatbot
            response = await chatbot.process_query(
                query=enhanced_query,
                user_data=user_data
            )
            
            # Start streaming the response
            yield f"data: {json.dumps({'type': 'log', 'content': '‚ú® Generating response...'})}\n\n"
            await asyncio.sleep(0.1)
            
            # Stream response content word by word
            response_content = response.get('response', 'I apologize, but I encountered an issue processing your request.')
            words = response_content.split()
            
            for i, word in enumerate(words):
                yield f"data: {json.dumps({'type': 'content', 'content': word + ' '})}\n\n"
                await asyncio.sleep(0.03)  # Adjust speed as needed
            
            # Add sources if available
            sources = response.get('sources', [])
            if sources:
                yield f"data: {json.dumps({'type': 'log', 'content': f'üìä Response based on {len(sources)} sources'})}\n\n"
                await asyncio.sleep(0.1)
                
                sources_text = f"{newline}{newline}üìö **Sources:**{newline}"
                for i, source in enumerate(sources[:3], 1):
                    source_title = source.get('title', 'Source')
                    sources_text += f"{i}. {source_title}{newline}"
                
                # Stream sources
                for word in sources_text.split():
                    yield f"data: {json.dumps({'type': 'content', 'content': word + ' '})}\n\n"
                    await asyncio.sleep(0.02)
             
            # Save conversation history
            if request.conversation_id:
                save_conversation_history(request.conversation_id, [
                    {"role": "user", "content": request.query},
                    {"role": "assistant", "content": response_content}
                ])
                yield f"data: {json.dumps({'type': 'log', 'content': 'üíæ Conversation saved'})}\n\n"
            
            # End of stream
            yield f"data: [DONE]\n\n"
             
        except HTTPException as http_e:
            logger.error(f"‚ùå HTTP error in chat streaming: {http_e}")
            error_msg = json.dumps({"type": "error", "content": f"Request error: {http_e.detail}"})
            yield f"data: {error_msg}\n\n"
            yield f"data: [DONE]\n\n"
        except asyncio.TimeoutError:
            logger.error("‚ùå Chat streaming timeout")
            error_msg = json.dumps({"type": "error", "content": "Chat response timed out. Please try again."})
            yield f"data: {error_msg}\n\n"
            yield f"data: [DONE]\n\n"
        except Exception as e:
            logger.error(f"‚ùå Critical error in chat streaming: {e}")
            # Try to provide a fallback response
            try:
                fallback_response = chatbot._generate_fallback_response(request.query, pdf_context=pdf_context_text)
                yield f"data: {json.dumps({'type': 'content', 'content': fallback_response})}\n\n"
                yield f"data: {json.dumps({'type': 'sources', 'sources': []})}\n\n"
            except:
                error_msg = json.dumps({"type": "error", "content": "I apologize, but I'm experiencing technical difficulties. Please try again."})
                yield f"data: {error_msg}\n\n"
            yield f"data: [DONE]\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type"
        }
    )

# PDF-enhanced chat endpoint
@app.post("/api/chat/pdf")
async def pdf_enhanced_chat(request: ChatRequest):
    """
    Chat endpoint with PDF context integration
    Processes user queries with uploaded PDF financial data context
    """
    try:
        logger.info(f"üìÑ PDF-enhanced chat request: {request.message[:100]}...")
        
        # Validate PDF context
        if not request.pdf_context:
            raise HTTPException(
                status_code=400, 
                detail="PDF context is required for this endpoint"
            )
        
        # Load existing conversation history if conversation_id provided
        if request.conversation_id and not request.conversation_history:
            request.conversation_history = await load_conversation_history(request.conversation_id)
        
        # Track demo mode sessions
        if request.demo_mode and request.conversation_id:
            _demo_mode_sessions.add(request.conversation_id)
        
        # Format PDF context for AI analysis
        pdf_context_text = format_pdf_context_for_ai(request.pdf_context)
        
        # Process query with PDF context
        result = await chat_system.process_query(
            query=request.message,
            user_id=request.user_id,
            conversation_id=request.conversation_id,
            think_mode=request.think_mode,
            agent=request.agent,
            demo_mode=request.demo_mode,
            pdf_context=pdf_context_text
        )
        
        # Save conversation history
        if request.conversation_id and result.get('response'):
            await save_conversation_history(
                conversation_id=request.conversation_id,
                message=request.message,
                response=result.get('response', ''),
                user_id=request.user_id
            )
        
        # Add PDF metadata to response
        result['pdf_enhanced'] = True
        result['pdf_summary'] = {
            "document_type": request.pdf_context.get('document_type', 'Unknown'),
            "accounts_count": len(request.pdf_context.get('accounts', [])),
            "transactions_count": len(request.pdf_context.get('transactions', [])),
            "confidence_score": request.pdf_context.get('confidence_score', 0.0)
        }
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå PDF-enhanced chat failed: {e}")
        raise HTTPException(status_code=500, detail=f"PDF-enhanced chat failed: {str(e)}")


# Non-streaming chat endpoint
@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """Non-streaming chat endpoint with conversation history"""
    try:
        # Load conversation history if conversation_id is provided
        if request.conversation_id and not request.conversation_history:
            request.conversation_history = await load_conversation_history(request.conversation_id)
        
        # Track demo mode sessions
        if request.demo_mode and request.conversation_id:
            _demo_mode_sessions.add(request.conversation_id)
        
        # Prepare PDF context if available
        pdf_context = None
        if request.pdf_context:
            pdf_context = format_pdf_context_for_ai(request.pdf_context)
        
        result = await chat_system.process_query(
            query=request.message,
            user_id=request.user_id,
            conversation_id=request.conversation_id,
            think_mode=request.think_mode,
            agent=request.agent,
            demo_mode=request.demo_mode,
            pdf_context=pdf_context
        )
        
        # Save conversation history
        if request.conversation_id and result.get('response'):
            await save_conversation_history(
                conversation_id=request.conversation_id,
                message=request.message,
                response=result.get('response', ''),
                user_id=request.user_id
            )
        
        return result
    except Exception as e:
        logger.error(f"‚ùå Chat endpoint failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Legacy query endpoint (maintained for compatibility)
@app.post("/query")
async def query_endpoint(request: QueryRequest):
    """Legacy query endpoint for backward compatibility with conversation history"""
    try:
        # Track demo mode sessions
        if request.demo_mode and request.conversation_id:
            _demo_mode_sessions.add(request.conversation_id)
        
        # Prepare PDF context if available
        pdf_context = None
        if hasattr(request, 'user_data') and request.user_data and 'pdf_context' in request.user_data:
            pdf_context = format_pdf_context_for_ai(request.user_data['pdf_context'])
        
        result = await chat_system.process_query(
            query=request.query,
            user_id=request.user_id,
            conversation_id=request.conversation_id,
            think_mode=request.think_mode,
            agent=request.agent,
            demo_mode=request.demo_mode,
            pdf_context=pdf_context
        )
        
        # Save conversation history
        if request.conversation_id and result.get('response'):
            await save_conversation_history(
                conversation_id=request.conversation_id,
                message=request.query,
                response=result.get('response', ''),
                user_id=request.user_id
            )
        
        return result
    except Exception as e:
        logger.error(f"‚ùå Query endpoint failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Global exception handlers
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"‚ùå Global exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error", "error": str(exc)}
    )


@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail}
    )


# User profile save endpoint
@app.post("/api/user/save")
async def save_user_profile_endpoint(request: Request):
    """Save user profile data"""
    try:
        data = await request.json()
        logger.info(f"üíæ Saving user profile: {data.get('email', 'unknown')}")
        
        # Extract user data from request
        user_data = {
            "firstName": data.get("firstName", ""),
            "lastName": data.get("lastName", ""),
            "email": data.get("email", ""),
            "phone": data.get("phone", ""),
            "dateOfBirth": data.get("dateOfBirth", ""),
            "occupation": data.get("occupation", ""),
            "annualIncome": data.get("annualIncome", ""),
            "riskTolerance": data.get("riskTolerance", "moderate"),
            "investmentGoals": data.get("investmentGoals", [])
        }
        
        # Generate user_id from email if available
        user_id = None
        if user_data["email"] and USER_MODELS_AVAILABLE:
            try:
                user_id = create_user_id_from_email(user_data["email"])
                user_data["user_id"] = user_id
                
                # Save to database if available
                save_result = save_user_profile(user_id, user_data)
                if save_result:
                    logger.info(f"‚úÖ User profile saved to database: {user_id}")
                else:
                    logger.warning(f"‚ö†Ô∏è Failed to save user profile to database: {user_id}")
            except Exception as db_error:
                logger.error(f"‚ùå Database error saving user profile: {db_error}")
        
        return {
            "success": True,
            "message": "User profile saved successfully",
            "user_id": user_id,
            "data": user_data
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error saving user profile: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to save user profile: {str(e)}")


# Fi Money Authentication endpoints
@app.post("/api/fi-auth/initiate")
async def initiate_fi_auth():
    """Initiate Fi Money authentication"""
    try:
        logger.info("üåê Initiating Fi Money authentication...")
        
        if not FI_MONEY_AVAILABLE:
            logger.warning("‚ö†Ô∏è Fi Money service not available")
            return {
                "status": "error",
                "message": "Fi Money service is not available"
            }
        
        # Check if already authenticated
        try:
            from integrations.fi_money_client import fi_money_client
            auth_status = await fi_money_client.check_authentication()
            
            if auth_status.get("authenticated", False):
                logger.info("‚úÖ Already authenticated with Fi Money")
                return {
                    "status": "already_authenticated",
                    "message": "Already authenticated with Fi Money"
                }
        except Exception as check_error:
            logger.warning(f"‚ö†Ô∏è Could not check existing auth status: {check_error}")
        
        # Initiate new authentication
        try:
            # Mock Fi Money authentication initiation for now
            # TODO: Implement actual Fi Money MCP integration
            logger.info("üîó Fi Money login URL generated (mock implementation)")
            return {
                "status": "login_required",
                "login_url": "https://fi.money/auth/mock",
                "session_id": "mock_session_123",
                "message": "Please complete authentication via the provided URL (mock)"
            }
        except Exception as init_error:
            logger.error(f"‚ùå Fi Money authentication initiation error: {init_error}")
            return {
                "status": "error",
                "message": f"Authentication service error: {str(init_error)}"
            }
            
    except Exception as e:
        logger.error(f"‚ùå Error in Fi Money authentication initiation: {e}")
        raise HTTPException(status_code=500, detail=f"Authentication initiation failed: {str(e)}")


@app.get("/api/fi-auth/status")
async def check_fi_auth_status():
    """Check Fi Money authentication status"""
    try:
        logger.info("üîç Checking Fi Money authentication status...")
        
        if not FI_MONEY_AVAILABLE:
            logger.warning("‚ö†Ô∏è Fi Money service not available")
            return {
                "status": "error",
                "message": "Fi Money service is not available",
                "auth_status": {
                    "authenticated": False
                }
            }
        
        try:
            # Mock Fi Money authentication status check for now
            # TODO: Implement actual Fi Money MCP integration
            logger.info("üìä Fi Money auth status checked (mock implementation)")
            
            return {
                "status": "success",
                "auth_status": {
                    "authenticated": False,
                    "expires_in_minutes": None,
                    "message": "Authentication status checked (mock - not authenticated)"
                }
            }
            
        except Exception as check_error:
            logger.error(f"‚ùå Fi Money authentication status check error: {check_error}")
            return {
                "status": "error",
                "message": f"Status check failed: {str(check_error)}",
                "auth_status": {
                    "authenticated": False
                }
            }
            
    except Exception as e:
        logger.error(f"‚ùå Error checking Fi Money authentication status: {e}")
        raise HTTPException(status_code=500, detail=f"Status check failed: {str(e)}")


@app.post("/api/fi-auth/complete")
async def complete_fi_auth():
    """Complete Fi Money authentication process"""
    try:
        logger.info("üîÑ Fi Money authentication completion requested...")
        
        if not FI_MONEY_AVAILABLE:
            logger.warning("‚ö†Ô∏è Fi Money service not available")
            return {
                "status": "error",
                "message": "Fi Money service is not available",
                "auth_status": {
                    "authenticated": False
                }
            }
        
        try:
            # Mock Fi Money authentication completion for now
            # TODO: Implement actual Fi Money MCP integration
            logger.info("üìä Fi Money auth completion checked (mock implementation)")
            
            return {
                "status": "success",
                "auth_status": {
                    "authenticated": False,
                    "expires_in_minutes": None,
                    "message": "Authentication completion checked (mock - not authenticated)"
                },
                "message": "Authentication completion checked successfully"
            }
            
        except Exception as complete_error:
            logger.error(f"‚ùå Fi Money authentication completion error: {complete_error}")
            return {
                "status": "error",
                "message": f"Completion check failed: {str(complete_error)}",
                "auth_status": {
                    "authenticated": False
                }
            }
            
    except Exception as e:
        logger.error(f"‚ùå Error in Fi Money authentication completion: {e}")
        raise HTTPException(status_code=500, detail=f"Authentication completion failed: {str(e)}")


@app.get("/api/cache/status")
async def get_cache_status(email: str):
    """Check if user has valid cached financial data"""
    try:
        logger.info(f"üîç Checking cache status for email: {email}")
        
        # Mock cache status response for now
        # TODO: Implement actual cache system integration
        logger.info("üìä Cache status checked (mock implementation)")
        
        return {
            "enabled": False,
            "has_cache": False,
            "expires_at": None,
            "time_remaining": None,
            "cached_at": None,
            "message": "Secure cache system is not enabled (mock)"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error checking cache status: {e}")
        raise HTTPException(status_code=500, detail=f"Cache status check failed: {str(e)}")


# Status endpoint
@app.get("/api/status")
async def get_status():
    """Get server status"""
    return {
        "status": "running",
        "version": "2.0",
        "timestamp": datetime.now().isoformat(),
        "features": {
            "gemini_ai": GEMINI_AVAILABLE,
            "fi_money": FI_MONEY_AVAILABLE,
            "investment_agent": INVESTMENT_AGENT_AVAILABLE,
            "pdf_support": SERVICES_AVAILABLE,
            "session_history": ROUTERS_AVAILABLE,
            "conversation_persistence": True,
            "demo_mode_tracking": True,
            "pdf_context_injection": True,
            "pdf_enhanced_chat": True
        },
        "active_demo_sessions": len(_demo_mode_sessions)
    }


if __name__ == "__main__":
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )

